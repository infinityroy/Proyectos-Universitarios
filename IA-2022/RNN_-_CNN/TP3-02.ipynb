{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ced7cff5",
   "metadata": {},
   "source": [
    "# Sección 2. Análisis de sentimientos con redes neuronales recurrentes LSTM utilizando Pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c518c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas requeridas\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b1b10f",
   "metadata": {},
   "source": [
    "## Problema\n",
    "\n",
    "Se desea que, dado un comentario de crítica acerca de la experiencia vivida dentro de un hotel determinado, predecir la calificación correspondiente a ese comentario. La calificación va del 1 al 5 siendo 5 la mejor calificación y 1 la peor calificación. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a8f33e",
   "metadata": {},
   "source": [
    "## Objetivo\n",
    "1. Poner en práctica habilidades de investigación y documentación de resultados.\n",
    "2. Aplicar el conocimiento teórico sobre redes neuronales recurrentes LSTM.\n",
    "3. Experimentar con el flujo completo de trabajo requerido en proyectos de\n",
    "aprendizaje automático para realizar análisis de sentimientos a partir de datos en\n",
    "lenguaje natural.\n",
    "4. Fortalecer capacidades en los estudiantes en el uso de bibliotecas de aprendizaje\n",
    "automático como PyTorch y otras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbd8af7",
   "metadata": {},
   "source": [
    "## Descripción de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13454a80",
   "metadata": {},
   "source": [
    "Los datos vienen dentro de un archivo csv que contiene 2 columnas, la primer columna es el comentario que se le realizó a una hotel determianda y la segunda columna trae la evaluación que se le dió al hotel. La evaluación que se le puede dar al comentario es de un rango del 1 al 5.\n",
    "\n",
    "Los datos utilizado en el ejemplo estás disponibles en https://www.kaggle.com/code/wiktorbrk/trip-advisor-reviews-sentiment-analysis/data. La descripción del sitio indica \"este es un conjunto de datos que contiene un comentario hacia un hotel y una calificación por parte del usuario\"\n",
    "\n",
    "El conjunto de datos consta de las siguientes columnas:\n",
    "\n",
    "Review: Un texto que es el comentario\n",
    "Rating: Numero positivo del 1 al 5 que indica la calificación al hotel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a0b292",
   "metadata": {},
   "source": [
    "## Carga y preparación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a592353e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nice hotel expensive parking got good deal sta...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok nothing special charge diamond member hilto...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nice rooms not 4* experience hotel monaco seat...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unique, great stay, wonderful time hotel monac...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great stay great stay, went seahawk game aweso...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  nice hotel expensive parking got good deal sta...       4\n",
       "1  ok nothing special charge diamond member hilto...       2\n",
       "2  nice rooms not 4* experience hotel monaco seat...       3\n",
       "3  unique, great stay, wonderful time hotel monac...       5\n",
       "4  great stay great stay, went seahawk game aweso...       5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leer los datos de ejemplos\n",
    "reviews = pd.read_csv(\"hotel_reviews.csv\")\n",
    "#print(reviews.shape)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0661942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cambio de la numeración de la clasificaciones de 0 a 4\n",
    "zero_numbering = {1:0, 2:1, 3:2, 4:3, 5:4}\n",
    "reviews['Rating'] = reviews['Rating'].apply(lambda x: zero_numbering[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab86b28",
   "metadata": {},
   "source": [
    "## Estadísticas de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b4317ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20491.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Rating\n",
       "count 20491.00\n",
       "mean      2.95\n",
       "std       1.23\n",
       "min       0.00\n",
       "25%       2.00\n",
       "50%       3.00\n",
       "75%       4.00\n",
       "max       4.00"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estadísticas de los datos\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "reviews.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "842d0c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({3: 6039, 1: 1793, 2: 2184, 4: 9054, 0: 1421})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificación de qué tan bien balanceadas están las clases.\n",
    "Counter(reviews['Rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d1abb5",
   "metadata": {},
   "source": [
    "## Creación definición de modelo y definición de sus datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71a57f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenización: proceso de separar un fragmento de texto en \n",
    "#  unidades más pequeñas llamadas tokens. \n",
    "#  Los tokens pueden ser palabras, caracteres o sub-palabras.\n",
    "tok = spacy.blank(\"en\")\n",
    "\n",
    "def tokenize (text):\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n",
    "    nopunct = regex.sub(\" \", text.lower())\n",
    "    return [token.text for token in tok.tokenizer(nopunct)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12f1f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se cuenta la cantidad de ocurrencias de cada token \n",
    "# en el corpus.\n",
    "\n",
    "#count number of occurences of each word\n",
    "counts = Counter()\n",
    "for index, row in reviews.iterrows():\n",
    "    counts.update(tokenize(row['Review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a1fe997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea el vocabulario\n",
    "vocab2index = {\"\":0, \"UNK\":1}\n",
    "words = [\"\", \"UNK\"]\n",
    "for word in counts:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)\n",
    "\n",
    "#print(vocab2index)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2afcb37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(text, vocab2index, N=70):\n",
    "    \"\"\"\n",
    "    Codificación de una oración antes de ser utilizada por el modelo. \n",
    "    Parámetros:\n",
    "       text: el texto a procesar\n",
    "       vocab2index: diccionario con el vocabulario a utilizar. \n",
    "       N: largo máximo\n",
    "    \"\"\"\n",
    "    tokenized = tokenize(text)\n",
    "    encoded = np.zeros(N, dtype=int)\n",
    "    \n",
    "    # El get en diccionario permite definir un valor si un item no existe (\"UNK\").  \n",
    "    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n",
    "    \n",
    "    # Largo máximo del resultado.\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    return encoded, length\n",
    "\n",
    "reviews['encoded'] = reviews['Review'].apply(lambda x: np.array(encode_sentence(x,vocab2index ), dtype=object))\n",
    "#print(reviews.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13dbc5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracción de características y target.\n",
    "X = list(reviews['encoded'])\n",
    "y = list(reviews['Rating'])\n",
    "\n",
    "# División de datos de entrenamiento y validación\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d065b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definción de la clase Dataset para manejo de los datos\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx], self.X[idx][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3bac86b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_fixed_len(torch.nn.Module) :\n",
    "    \"\"\"\n",
    "    Clase para realizar la clasificación de las oraciones. \n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, tagset_size=5) :\n",
    "        \"\"\"\n",
    "        Inicialización de la clase.\n",
    "        Parámetros:\n",
    "           embedding_dim: dimesionalidad del vector de palabras. \n",
    "           hidden_dim: dimensión de la capa oculta de la red. \n",
    "           vocab_size: tamaño del vocabulario.  \n",
    "           tagset_size: número de clases.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, tagset_size)\n",
    "        \n",
    "        # Durante el entrenamiento, pone a cero aleatoriamente algunos de los elementos \n",
    "        # del tensor de entrada con probabilidad p utilizando muestras de una \n",
    "        # distribución de Bernoulli. Esta ha demostrado ser una técnica eficaz para la regularización.\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return self.linear(ht[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "00948a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(words)\n",
    "\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "# Instancia del modelo\n",
    "model_fixed = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, 50, 50)\n",
    "#model_fixed =  LSTM_fixed_len(vocab_size, 50, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13647d6",
   "metadata": {},
   "source": [
    "## Definición de los hiper-parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "30f7b32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "#Funcion de loss\n",
    "loss_function = F.cross_entropy\n",
    "\n",
    "#Funcipon de optimización\n",
    "parameters = filter(lambda p: p.requires_grad, model_fixed.parameters())\n",
    "optimizer = torch.optim.SGD(parameters, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533999b0",
   "metadata": {},
   "source": [
    "## Datos de pruebas y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "90d83eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de los datasets de entrenamiento y validación\n",
    "\n",
    "batch_size = 5000\n",
    "\n",
    "train_ds = ReviewsDataset(X_train, y_train)\n",
    "valid_ds = ReviewsDataset(X_valid, y_valid)\n",
    "\n",
    "#Datos de entrenamiento y validación\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1a03887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardo para mostar la gráfica de error\n",
    "acc_l = []\n",
    "loss_l = []\n",
    "val_acc_l = []\n",
    "val_loss_l = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4bf50f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esta funcion me ayuda a extraer datos para poder graficar la curva de error\n",
    "def validation_metrics (model, valid_dl):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    sum_rmse = 0.0\n",
    "    for x, y, l in valid_dl:\n",
    "        x = x.long()\n",
    "        y = y.long()\n",
    "        y_hat = model(x, l)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        pred = torch.max(y_hat, 1)[1]\n",
    "        correct += (pred == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "        sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1)))*y.shape[0]\n",
    "    return sum_loss/total, correct/total, sum_rmse/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91478698",
   "metadata": {},
   "source": [
    "## Entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5a536de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,train_dl,epochs):\n",
    "    \"\"\"\n",
    "    Entrenamiento del modelo utilizando PyTorch.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    #optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, y, l in train_dl:\n",
    "            x = x.long()\n",
    "            y = y.long()\n",
    "            y_pred = model(x, l)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_function(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            #loss_values.append(loss_l)\n",
    "            total += y.shape[0]\n",
    "            \n",
    "            res=y       \n",
    "        \n",
    "        val_loss, val_acc, val_rmse = validation_metrics(model, val_dl)\n",
    "        \n",
    "        #guardo datos para graficarlos en curva de error\n",
    "        val_loss_l.append(val_loss)\n",
    "        val_acc_l.append(val_acc)\n",
    "        loss_l.append(sum_loss/total)\n",
    "        acc_l.append(val_rmse)\n",
    "        \n",
    "        if i % 5 == 1:\n",
    "            print(\"train loss %.3f, val loss %.3f, val accuracy %.3f, and val rmse %.3f\" % (sum_loss/total, val_loss, val_acc, val_rmse))\n",
    "    print(\"Termina el entrenamiento\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5f5876fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [54]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prediccion \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_fixed\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [53]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dl, epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m     13\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m---> 14\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(y_pred, y)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "prediccion = train_model(model_fixed,train_dl, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a11fe91",
   "metadata": {},
   "source": [
    "## Gráfica de curva de error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a512c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "epochs = range(len(acc_l))\n",
    "\n",
    "loss = loss_l\n",
    "val_loss = val_loss_l\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5fa403",
   "metadata": {},
   "source": [
    "## Evaluación del modelo - Acurrancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718ab2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validación del modelo \n",
    "average_loss, accuracy, average_rmse = validation_metrics (model_fixed, val_dl)\n",
    "\n",
    "print (\"Exactitud: \", accuracy * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ed824f",
   "metadata": {},
   "source": [
    "## Evaluación del modelo - Matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcac2bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#y_pred = model_fixed(a,c)\n",
    "#una lista con calificaciones del 1 al 5\n",
    "\n",
    "y_true = y_train[15000:] #los últimos 1392 son los resultados correctos de los targets\n",
    "y_pred = prediccion\n",
    "\n",
    "#y_pred = model(x, l)\n",
    "#y_pred = y_train\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize = (5,5))\n",
    "sns.heatmap(cm,cmap= \"Blues\", \n",
    "            linecolor = 'black', \n",
    "            linewidth = 1, \n",
    "            annot = True, \n",
    "            fmt='', \n",
    "            xticklabels = ['1','2','3','4','5'], \n",
    "            yticklabels = ['1','2','3','4','5'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed5411f",
   "metadata": {},
   "source": [
    "Observando la matriz de confusión se puede deducir que el modelo no está muy bien entrenado, ya que se confunde entre clases. Otra observación es que aunque el modelo no esté muy bien entrenado, predice correctamente la mayoría de manera muy buena."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4429961",
   "metadata": {},
   "source": [
    "## Mejoras al modelo - Balanceo de carga (Clase con menor datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbe5521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardo para mostar la gráfica de error\n",
    "acc_l = []\n",
    "loss_l = []\n",
    "val_acc_l = []\n",
    "val_loss_l = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bd62c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Balanceo las clases del csv utilizando la clase que tiene menos datos\n",
    "\n",
    "df = pd.read_csv(\"hotel_reviews.csv\")\n",
    "\n",
    "a = pd.DataFrame(df[df.Rating == 1][:1420])\n",
    "b = pd.DataFrame(df[df.Rating == 2][:1420])\n",
    "c = pd.DataFrame(df[df.Rating == 3][:1420])\n",
    "d = pd.DataFrame(df[df.Rating == 4][:1420])\n",
    "e = pd.DataFrame(df[df.Rating == 5][:1420])\n",
    "\n",
    "frames = [a,b,c,d,e]\n",
    "result = pd.concat(frames)\n",
    "\n",
    "#Cambio de la numeración de la clasificaciones de 0 a 4\n",
    "zero_numbering = {1:0, 2:1, 3:2, 4:3, 5:4}\n",
    "result['Rating'] = result['Rating'].apply(lambda x: zero_numbering[x])\n",
    "\n",
    "# Verificación de qué tan bien balanceadas están las clases.\n",
    "Counter(result['Rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cc097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['encoded'] = result['Review'].apply(lambda x: np.array(encode_sentence(x,vocab2index ), dtype=object))\n",
    "# Extracción de características y target.\n",
    "X2 = list(result['encoded'])\n",
    "y2 = list(result['Rating'])\n",
    "\n",
    "# División de datos de entrenamiento y validación\n",
    "X_train2, X_valid2, y_train2, y_valid2 = train_test_split(X2, y2, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeae261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de los datasets de entrenamiento y validación\n",
    "\n",
    "batch_size = 5000\n",
    "\n",
    "train_ds2 = ReviewsDataset(X_train2, y_train2)\n",
    "valid_ds2 = ReviewsDataset(X_valid2, y_valid2)\n",
    "\n",
    "#Datos de entrenamiento y validación\n",
    "train_dl2 = DataLoader(train_ds2, batch_size=batch_size, shuffle=True)\n",
    "val_dl2 = DataLoader(valid_ds2, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0737a8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fixed2 =  LSTM_fixed_len(len(words),50,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b199dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion2 = train_model(model_fixed2,train_dl2, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0861c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validación del modelo \n",
    "average_loss, accuracy2, average_rmse = validation_metrics (model_fixed2, val_dl2)\n",
    "print (\"Exactitud: \", accuracy * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bacdf48",
   "metadata": {},
   "source": [
    "Al principio nuestro modelo nos daba un rendimiento bajo, aunque con esta \"mejora\" nos haya dado parecido, el entrenamiento lo ha realizado en menos tiempo y con un resultado favorable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d1ae7a",
   "metadata": {},
   "source": [
    "## Mejoras al modelo - (Aumento de epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1376f51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion = train_model(model_fixed,train_dl, epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799875d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validación del modelo \n",
    "average_loss, accuracy, average_rmse = validation_metrics (model_fixed, val_dl)\n",
    "print (\"Exactitud: \", accuracy * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b7d55b",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "- La exactitud indica que el modelo no logra buenos resultados, sin embargo, el RMSE que es la desviación estándar de los residuos (errores de predicción), que en otras palabras, representa qué tan concentrados están los datos alrededor de la línea de mejor ajuste, está alrededor de 1 punto lo que indica que las predicciones no son tan malas.\n",
    "\n",
    "- Las clases no están desbalanceadas como se muestra al principio. Se podría variar la composición por ejemplo fusionando algunas y evaluar si eso mejora el rendimiento.\n",
    "\n",
    "- Antes de clasificar textos en una red LSTM se debe revisar el texto ya que puede contener datos innecesarios, osea, realizar un preprocesamiento eliminando texto innecesario.\n",
    "\n",
    "- Se deben de configurar correctamente los parámetros de la función de entrenamiento y de la red, esto ya que un debalance de estos produce retardos o mal entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957a8e84",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "- Sentiment Analysis of IMDB Movie Reviews. Kaggle.com. (2022). Retrieved 18 May 2022, from https://www.kaggle.com/code/lakshmi25npathi/sentiment-analysis-of-imdb-movie-reviews/data.\n",
    "- Sentiment Analysis of IMDB Movie Reviews. Kaggle.com. (2022). Retrieved 18 May 2022, from https://www.kaggle.com/code/lakshmi25npathi/sentiment-analysis-of-imdb-movie-reviews/notebook.\n",
    "- animal-image-classifications/I_notebook.ipynb at master · imamun93/animal-image-classifications. GitHub. (2022). Retrieved 18 May 2022, from https://github.com/imamun93/animal-image-classifications/blob/master/I_notebook.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424368af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
